{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation of the train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = pd.read_csv('all-gendered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trans[trans['Physiological']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('all-gendered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the original train set has some duplicates, which should be kept in the final train set\n",
    "eda_train = train[train['Label'].notna()].copy()\n",
    "eda_train = eda_train[eda_train['Sentence'].notna()]\n",
    "\n",
    "eda_train = eda_train.drop_duplicates(subset=['Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = eda_train[eda_train['Label']==0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energyloss = eda_train[eda_train['Loss_of_energy']==1].copy()\n",
    "agitation = eda_train[eda_train['Agitation']==1].copy()\n",
    "sadness = eda_train[eda_train['Sadness']==1].copy()\n",
    "irritability = eda_train[eda_train['Irritability']==1].copy()\n",
    "socialwithdr = eda_train[eda_train['Social_withdrawal']==1].copy()\n",
    "failsense = eda_train[eda_train['Sense_of_failure']==1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns[26:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affective = eda_train[eda_train['Affective']==1].copy()\n",
    "motivational = eda_train[eda_train['Motivational']==1].copy()\n",
    "cognitive = eda_train[eda_train['Cognitive']==1].copy()\n",
    "cog_distortions = eda_train[eda_train['Cog_distortions']==1].copy()\n",
    "behavioral = eda_train[eda_train['Behavioral']==1].copy()\n",
    "physiological = eda_train[eda_train['Physiological']==1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in train.columns[26:32]:\n",
    "    count = train[train[name]==1]['Gender'].value_counts()\n",
    "    print(name, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_control = control[control['Gender']==0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_affective = affective[affective['Gender']==0].copy()\n",
    "male_motivational = motivational[motivational['Gender']==0].copy()\n",
    "fem_cognitive = cognitive[cognitive['Gender']==1].copy()\n",
    "male_cog_distortions = cog_distortions[cog_distortions['Gender']==0].copy()\n",
    "male_behavioral = behavioral[behavioral['Gender']==0].copy()\n",
    "male_physiological = physiological[physiological['Gender']==0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_control = male_control.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energyloss = energyloss.reset_index(drop=True)\n",
    "agitation = agitation.reset_index(drop=True)\n",
    "sadness = sadness.reset_index(drop=True)\n",
    "irritability = irritability.reset_index(drop=True)\n",
    "socialwithdr = socialwithdr.reset_index(drop=True)\n",
    "fem_socialwithdr = fem_socialwithdr.reset_index(drop=True)\n",
    "male_failsense = male_failsense.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_affective = male_affective.reset_index(drop=True)\n",
    "male_motivational = male_motivational.reset_index(drop=True)\n",
    "fem_cognitive = fem_cognitive.reset_index(drop=True)\n",
    "male_cog_distortions = male_cog_distortions.reset_index(drop=True)\n",
    "male_behavioral = male_behavioral.reset_index(drop=True)\n",
    "male_physiological = male_physiological.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA implementation taken from Jason Wei and Kai Zou**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy data augmentation techniques for text classification\n",
    "# Jason Wei and Kai Zou\n",
    "import random\n",
    "from random import shuffle\n",
    "random.seed(1)\n",
    "\n",
    "#stop words list\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
    "\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n",
    "\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n",
    "\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n",
    "\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n",
    "\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n",
    "\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n",
    "\t\t\t'into', 'through', 'during', 'before', 'after', \n",
    "\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n",
    "\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n",
    "\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
    "\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "\t\t\t'should', 'now', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up text\n",
    "import re\n",
    "def get_only_chars(line):\n",
    "\n",
    "    clean_line = \"\"\n",
    "\n",
    "    line = line.replace(\"â€™\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Synonym replacement\n",
    "# Replace n words in the sentence with synonyms from wordnet\n",
    "########################################################################\n",
    "\n",
    "#for the first time you use wordnet\n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n: #only replace up to n words\n",
    "\t\t\tbreak\n",
    "\n",
    "\t#this is stupid but we need it, trust me\n",
    "\tsentence = ' '.join(new_words)\n",
    "\tnew_words = sentence.split(' ')\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynonyms = set()\n",
    "\tfor syn in wordnet.synsets(word): \n",
    "\t\tfor l in syn.lemmas(): \n",
    "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\tsynonyms.add(synonym) \n",
    "\tif word in synonyms:\n",
    "\t\tsynonyms.remove(word)\n",
    "\treturn list(synonyms)\n",
    "\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "\n",
    "def random_deletion(words, p):\n",
    "\n",
    "\t#obviously, if there's only one word, don't delete it\n",
    "\tif len(words) == 1:\n",
    "\t\treturn words\n",
    "\n",
    "\t#randomly delete words with probability p\n",
    "\tnew_words = []\n",
    "\tfor word in words:\n",
    "\t\tr = random.uniform(0, 1)\n",
    "\t\tif r > p:\n",
    "\t\t\tnew_words.append(word)\n",
    "\n",
    "\t#if you end up deleting all words, just return a random word\n",
    "\tif len(new_words) == 0:\n",
    "\t\trand_int = random.randint(0, len(words)-1)\n",
    "\t\treturn [words[rand_int]]\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "\n",
    "def random_swap(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tnew_words = swap_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
    "\trandom_idx_2 = random_idx_1\n",
    "\tcounter = 0\n",
    "\twhile random_idx_2 == random_idx_1:\n",
    "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter > 3:\n",
    "\t\t\treturn new_words\n",
    "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random insertion\n",
    "# Randomly insert n words into the sentence\n",
    "########################################################################\n",
    "\n",
    "def random_insertion(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tadd_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def add_word(new_words):\n",
    "\tsynonyms = []\n",
    "\tcounter = 0\n",
    "\twhile len(synonyms) < 1:\n",
    "\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter >= 10:\n",
    "\t\t\treturn\n",
    "\trandom_synonym = synonyms[0]\n",
    "\trandom_idx = random.randint(0, len(new_words)-1)\n",
    "\tnew_words.insert(random_idx, random_synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# main data augmentation function\n",
    "########################################################################\n",
    "\n",
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "\t\n",
    "\tsentence = get_only_chars(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word != '']\n",
    "\tnum_words = len(words)\n",
    "\t\n",
    "\taugmented_sentences = []\n",
    "\tnum_new_per_technique = int(num_aug/4)+1\n",
    "\n",
    "\t#sr\n",
    "\tif (alpha_sr > 0):\n",
    "\t\tn_sr = max(1, int(alpha_sr*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = synonym_replacement(words, n_sr)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#ri\n",
    "\tif (alpha_ri > 0):\n",
    "\t\tn_ri = max(1, int(alpha_ri*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_insertion(words, n_ri)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#rs\n",
    "\tif (alpha_rs > 0):\n",
    "\t\tn_rs = max(1, int(alpha_rs*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_swap(words, n_rs)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#rd\n",
    "\tif (p_rd > 0):\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_deletion(words, p_rd)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "\tshuffle(augmented_sentences)\n",
    "\n",
    "\t#trim so that we have the desired number of augmented sentences\n",
    "\tif num_aug >= 1:\n",
    "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
    "\telse:\n",
    "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
    "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "\t#append the original sentence\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy data augmentation techniques for text classification\n",
    "# Jason Wei and Kai Zou\n",
    "\n",
    "#generate more data with standard augmentation\n",
    "def gen_eda(train_orig, alpha_sr, alpha_ri, alpha_rs, alpha_rd, num_aug=9):\n",
    "    \n",
    "    augmented_rows = []\n",
    "    \n",
    "    for i in range(len(train_orig)):\n",
    "        sentence = train_orig.loc[i, 'Sentence']\n",
    "        aug_sentences = eda(sentence, alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, p_rd=alpha_rd, num_aug=num_aug)\n",
    "        for aug_sentence in aug_sentences:\n",
    "            new_inst = train_orig.iloc[i].copy()\n",
    "            new_inst['Sentence'] = aug_sentence\n",
    "            augmented_rows.append(new_inst)\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of augmented sentences to generate per original sentence\n",
    "aff_num_aug = 1\n",
    "mot_num_aug = 4 #for cv model 2\n",
    "cog_num_aug = 1\n",
    "cog_dist_num_aug = 4 #for cv model 2\n",
    "beh_num_aug = 2\n",
    "control_num_aug = 1\n",
    "#phys_num_aug = 1\n",
    "\n",
    "\n",
    "#how much to replace each word by synonyms\n",
    "alpha_sr = 0.05\n",
    "\n",
    "#how much to insert new words that are synonyms\n",
    "alpha_ri = 0.05\n",
    "\n",
    "#how much to swap words\n",
    "alpha_rs = 0\n",
    "\n",
    "#how much to delete words\n",
    "alpha_rd = 0\n",
    "\n",
    "if alpha_sr == alpha_ri == alpha_rs == alpha_rd == 0:\n",
    "     print('At least one alpha should be greater than zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmentation negative sentences for the GABDI-CV model\n",
    "male_control_aug_data = gen_eda(male_control, alpha_sr, alpha_ri, alpha_rs, alpha_rd, control_num_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_aff_aug_data = gen_eda(male_affective, alpha_sr, alpha_ri, alpha_rs, alpha_rd, aff_num_aug)\n",
    "male_mot_aug_data = gen_eda(male_motivational, alpha_sr, alpha_ri, alpha_rs, alpha_rd, mot_num_aug)\n",
    "fem_cognitive_aug_data = gen_eda(fem_cognitive, alpha_sr, alpha_ri, alpha_rs, alpha_rd, cog_num_aug)\n",
    "male_cog_distortions_aug_data = gen_eda(male_cog_distortions, alpha_sr, alpha_ri, alpha_rs, alpha_rd, cog_dist_num_aug)\n",
    "male_beh_aug_data = gen_eda(male_behavioral, alpha_sr, alpha_ri, alpha_rs, alpha_rd, beh_num_aug)\n",
    "#if GABDI CV model:\n",
    "male_phys_aug_data = gen_eda(male_physiological, alpha_sr, alpha_ri, alpha_rs, alpha_rd, phys_num_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_control_aug_data = male_control_aug_data.sample(125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for category model\n",
    "male_aff_aug_data = male_aff_aug_data.sample(113)\n",
    "male_mot_aug_data = male_mot_aug_data.sample(69)\n",
    "fem_cognitive_aug_data = fem_cognitive_aug_data.sample(12)\n",
    "male_cog_distortions_aug_data = male_cog_distortions_aug_data.sample(111)\n",
    "male_beh_aug_data = male_beh_aug_data.sample(63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for GABDI CV model\n",
    "male_aff_aug_data = male_aff_aug_data.sample(41)\n",
    "male_mot_aug_data = male_mot_aug_data.sample(54)\n",
    "fem_cognitive_aug_data = fem_cognitive_aug_data.sample(5)\n",
    "male_cog_distortions_aug_data = male_cog_distortions_aug_data.sample(48)\n",
    "male_beh_aug_data = male_beh_aug_data.sample(35)\n",
    "male_phys_aug_data = male_phys_aug_data.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category model\n",
    "aug_data = pd.concat([male_aff_aug_data, male_mot_aug_data, fem_cognitive_aug_data, \n",
    "                      male_cog_distortions_aug_data, male_beh_aug_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GABDI-CV model\n",
    "aug_data = pd.concat([male_aff_aug_data, male_mot_aug_data, fem_cognitive_aug_data, \n",
    "                      male_cog_distortions_aug_data, male_beh_aug_data, male_phys_aug_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fem_aug_data = fem_cognitive_aug_data.sample(12)\n",
    "#male_aug_data = male_cog_distortions_aug_data.sample(111, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aug_data = pd.concat([fem_aug_data, male_aug_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.concat([train, male_control_aug_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.concat([train, aug_data], ignore_index=True)\n",
    "#train_cognitive = pd.concat([train, fem_aug_data], ignore_index=True)\n",
    "#train_cog_distortions = pd.concat([train, male_aug_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_all.to_csv(\"cog_cogdist_syn_eda_train.csv\", index=False)\n",
    "#train_cognitive.to_csv('cog_syn_eda_train.csv', index=False)\n",
    "#train_cog_distortions.to_csv('cogdist_syn_eda_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_all.to_csv(\"fail_social_back_trans_train.csv\", index=False)\n",
    "#train_socialwithdr.to_csv('socialwithdr_back_trans_train.csv', index=False)\n",
    "#train_failsense.to_csv('failsense_back_trans_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
